{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e3e0ca",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook outlines the creation, compilation, and training of a deep learing network for audio classification using the [TorchSuite](https://github.com/sergio-sanz-rodriguez/torchsuite) framework.\n",
    " \n",
    "https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba7507",
   "metadata": {},
   "source": [
    "# 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6892a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torcheval\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "\n",
    "from pathlib import Path\n",
    "#from torchinfo import summary\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "#from torchaudio.transforms import Resample\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Import custom libraries\n",
    "from utils.classification_utils import set_seeds, predict_and_play_audio, load_model\n",
    "from engines.classification import ClassificationEngine\n",
    "from engines.schedulers import FixedLRSchedulerWrapper\n",
    "from dataloaders.audio_dataloaders import load_audio, create_dataloaders_waveform, PadWaveform, AudioWaveformTransforms\n",
    "from models.wav2vec2 import Wav2Vec2Classifier\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Paths (modify as needed)\n",
    "#TRAIN_DIR = Path(\"data/train\")\n",
    "#TEST_DIR = Path(\"data/validation\")\n",
    "INFERENCE_DIR = Path(\"train_soundscapes\")\n",
    "\n",
    "# Define some constants\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "SEED = 42\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(SEED)\n",
    "\n",
    "IMPORT_DATASET = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d316112",
   "metadata": {},
   "source": [
    "# 3. Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01afed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate cuda benchmark\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Set device\n",
    "device = \"cpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(\n",
    "        model: torch.nn.Module,\n",
    "        model_weights_dir: str,\n",
    "        model_weights_name: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Loads a PyTorch model from a target directory.\n",
    "\n",
    "    Args:\n",
    "    model: A target PyTorch model to load.\n",
    "    model_weights_dir: A directory where the model is located.\n",
    "    model_weights_name: The name of the model to load.\n",
    "      Should include either \".pth\" or \".pt\" as the file extension.\n",
    "\n",
    "    Example usage:\n",
    "    model = load_model(model=model,\n",
    "                       model_weights_dir=\"models\",\n",
    "                       model_weights_name=\"05_going_modular_tingvgg_model.pth\")\n",
    "\n",
    "    Returns:\n",
    "    The loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    # Create the model directory path\n",
    "    model_dir_path = Path(model_weights_dir)\n",
    "\n",
    "    # Create the model path\n",
    "    assert model_weights_name.endswith(\".pth\") or model_weights_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    model_path = model_dir_path / model_weights_name\n",
    "\n",
    "    # Load the model\n",
    "    print(f\"[INFO] Loading model from: {model_path}\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e87b9a",
   "metadata": {},
   "source": [
    "# 5. Preparing Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv('sample_submission.csv').drop([0, 1, 2])\n",
    "submission_labels = df_submission.columns[1:]\n",
    "submission_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c358ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('label_to_info.csv')\n",
    "idx_to_label = dict(zip(df_info[\"index\"], df_info[\"label\"]))\n",
    "idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3493e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CHUNK_DURATION_SEC = 5\n",
    "NEW_SAMPLE_RATE = 8000 # Hz\n",
    "TARGET_LENGTH = NEW_SAMPLE_RATE * CHUNK_DURATION_SEC # use 5-sec length\n",
    "#_, SAMPLE_RATE = load_audio(TRAIN_DIR / \"0\" / \"CSA36389_chunk0.wav\")\n",
    "BATCH_SIZE = 64\n",
    "ACCUM = 1\n",
    "NUM_CLASSES = df_info.shape[0]\n",
    "NUM_SAMPLES_PER_CLASS = 500\n",
    "VAL_PERCENTAGE = 0.2\n",
    "AUGMENT_MAGNITUDE = 2\n",
    "LR = 1e-5\n",
    "ETA_MIN = 1e-7\n",
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c8e7d",
   "metadata": {},
   "source": [
    "# 5. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load_model(\n",
    "    model = Wav2Vec2Classifier(num_classes=NUM_CLASSES),\n",
    "    model_weights_dir = MODEL_DIR,\n",
    "    model_weights_name = 'model_wave_8khz_pauc_epoch8.pth'\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "inference_context = torch.no_grad()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb41a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datafram to collect results\n",
    "results = []\n",
    "\n",
    "# Iterate over .ogg files\n",
    "for audio_path in INFERENCE_DIR.glob(\"*.ogg\"):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Load audio\n",
    "    waveform, SAMPLE_RATE = torchaudio.load(audio_path)\n",
    "\n",
    "    # Pre_processing \n",
    "    transform = AudioWaveformTransforms(\n",
    "        augmentation=False,   \n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        new_sample_rate=NEW_SAMPLE_RATE,\n",
    "        target_length=TARGET_LENGTH\n",
    "        )\n",
    "\n",
    "    # Compute the chunk duration in samples\n",
    "    MAX_CHUNK_DURATION_SAMPLES = CHUNK_DURATION_SEC * SAMPLE_RATE\n",
    "\n",
    "    # Ensure mono audio\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    total_duration_sec = waveform.shape[1] / SAMPLE_RATE\n",
    "    num_chunks = max(1, int(total_duration_sec // CHUNK_DURATION_SEC))\n",
    "\n",
    "    # Process each chunk\n",
    "    for i in range(num_chunks):\n",
    "        start_chunk = i * MAX_CHUNK_DURATION_SAMPLES\n",
    "        end_chunk = start_chunk + MAX_CHUNK_DURATION_SAMPLES\n",
    "        chunk = waveform[:, int(start_chunk):int(end_chunk)]        \n",
    "\n",
    "        # Check if the chunk is shorter than the desired duration\n",
    "        if chunk.shape[1] < MAX_CHUNK_DURATION_SAMPLES:\n",
    "            pad_size = MAX_CHUNK_DURATION_SAMPLES - chunk.shape[1]\n",
    "            chunk = F.pad(chunk, (0, pad_size))\n",
    "\n",
    "        chunk = chunk.to(device)\n",
    "        transform = transform.to(device)\n",
    "\n",
    "        with inference_context:\n",
    "            chunk = chunk.squeeze(1) if chunk.ndim == 3 else chunk            \n",
    "            chunk = transform(chunk)            \n",
    "            probs = torch.softmax(model(chunk), dim=1).cpu().numpy().flatten()\n",
    "        \n",
    "        # Generate row_id: filename without .ogg + _<chunk_end_time>\n",
    "        file_id = audio_path.stem\n",
    "        row_id = f\"{file_id}_{(i + 1) * CHUNK_DURATION_SEC}\"\n",
    "\n",
    "        label_to_prob = {str(idx_to_label[j]): probs[j] for j in range(len(probs))}        \n",
    "        ordered_probs = [label_to_prob.get(label, 0.0) for label in submission_labels]        \n",
    "        results.append([row_id] + ordered_probs)\n",
    "\n",
    "    # Convert to dataframe\n",
    "    submission_df = pd.DataFrame(results, columns=[\"row_id\"] + list(submission_labels))\n",
    "    display(submission_df)\n",
    "\n",
    "# Save to CSV\n",
    "#submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defdd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dcbebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefb12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838a40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a152c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc736cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98cfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3437a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9a63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43491842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da921f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695be21d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccca99a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
